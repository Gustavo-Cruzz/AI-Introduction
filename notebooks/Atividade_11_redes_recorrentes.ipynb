{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Atividade 11 -  redes recorrentes",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Capacitação Vialab # Atividade 11\n",
        "\n",
        "## Aprender sobre os difentes tipos de rede recorrentes\n",
        "### Data de atualização: 08/02/2022\n",
        "\n",
        "# Objetivo: Aprender conceitos e aplicações básicas de diferentes tipos de redes recorrentes"
      ],
      "metadata": {
        "id": "ar4fIgQoa_PJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2l-cVimzba5"
      },
      "source": [
        "# Predição de séries temporais utilizando redes neurais recorrentes simples,  LSTM e GRU\n",
        "\n",
        "\n",
        "Aqui desenvolveremos duas redes neurais recorrentes, dos tipos simples, LSTM e GRU, utilizando o Keras, para demonstrar sua capacidade em prever séries temporais\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPtf7Gzwzba9"
      },
      "source": [
        "%matplotlib inline\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import math\n",
        "import keras\n",
        "from tensorflow.keras import backend as K\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout, LSTM, GRU, Conv1D, MaxPooling1D, Flatten, SimpleRNN\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fody6193zbbC"
      },
      "source": [
        "Para garantir que nossos resultados sejam reprodutíveis, vamos fixar a semente de aleatorização (random seed) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlzYMaLMzbbE"
      },
      "source": [
        "# fixa random seed para garantir reprodutibilidade\n",
        "np.random.seed(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXkKXHkPzbbI"
      },
      "source": [
        "## Carregando os dados\n",
        "\n",
        "O dataset a ser utilizado é a produção de $CO_2$ em partes por milhão (ppm) do vulcão Mauna Loa\n",
        "(https://en.wikipedia.org/wiki/Mauna_Loa) entre 1965-1980 por mês. \n",
        "Baixe o CSV e mova ele para a mesma pasta deste notebook: https://github.com/gahjelle/data-analysis-with-python/blob/7305e2eecb6f3356539128703ad5d61e04a047c3/data/co2-ppm-mauna-loa-19651980.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGuHyCwhzbbK"
      },
      "source": [
        "Os dados serão carregados utilizando o Pandas. Não estamos interessados no campo \"data\", uma vez que cada observação está separada pelo mesmo intervalo de um mês. Assim, ao carregarmos os dados podemos excluir a primeira coluna. \n",
        "\n",
        "O arquivo CSV possui a informação no rodapé que pode ser também excluída (argumento pandas.read_csv() configurado para 3 para as últimas 3 linhas). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1g6dJZTIzbbL"
      },
      "source": [
        "# carrega o dataset\n",
        "df = pd.read_csv('./co2-ppm-mauna-loa-19651980.csv', usecols=[1], engine='python', skipfooter=3)\n",
        "data = df.values\n",
        "data = data.astype('float32')\n",
        "print('Quantidade de dados no arquivo:',len(data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlUmEbXPzbbP"
      },
      "source": [
        "## Visualizando os dados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkORh8DczbbR"
      },
      "source": [
        "plt.figure(figsize=[10,5])\n",
        "plt.xlabel('TimePoint in Months')\n",
        "plt.ylabel('$CO_2$(ppm)')\n",
        "plt.grid()\n",
        "plt.plot(data);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8ZbmRiJzbbV"
      },
      "source": [
        "Em séries temporais, a sequência dos dados é importante. Para criar a divisão entre treinamento e teste, utilizaremos a primeira parte da série para treinamento, e a última para o teste. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2NO9oBhzbbW"
      },
      "source": [
        "# split into train and test sets\n",
        "train_size = int(len(data) * 0.7)\n",
        "train, test = data[0:train_size,:], data[train_size:len(data),:]\n",
        "print('Dataset de treinamento contém: ', len(train),' dados')\n",
        "print('Dataset de teste contém: ', len(test),' dados')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAgW9WtNzbbc"
      },
      "source": [
        "Os LSTMs são sensíveis à escala dos dados de entrada, especificamente quando as funções de ativação sigmóide ou tanh são usadas. É fundamental normalizar os dados para o intervalo de [0, 1]. Isso pode ser feito usando a classe de pré-processamento MinMaxScaler da biblioteca scikit-learn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpnR0iB_zbbd"
      },
      "source": [
        "### Problema 1\n",
        "\n",
        "Use o MinMaxScaler para normalizar o conjunto de treinamento e testes entre 0 e 1. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Por que normalizar os dados?\n",
        "\n",
        "A normalização é um estágio de pré-processamento de dados. A normalização é um processo importante para a manipulação de dados, sendo assim, há remodelagem dos dados para uma escala numérica padrão. O processo que podemos é a escalabilidade de dados, podendo aumentá-la ou reduzi-la, antes de ser usado em outros estágios. A normalização é necessária quando os dados se encontram em diferentes escalas, caso contrário, pode levar a execuções ruins do modelo. Dito isso, há várias técnicas para realizar a normalização de dados.  "
      ],
      "metadata": {
        "id": "yDE9TPAnabGQ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hi8_nSLKzbbf"
      },
      "source": [
        "# normalize the dataset\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "data_train = scaler.fit_transform(data)\n",
        "norm_train = scaler.transform(train)\n",
        "norm_test = scaler.transform(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "SNOw5jI1zbbj"
      },
      "source": [
        "plt.figure(figsize=[10,5])\n",
        "plt.xlabel('Meses')\n",
        "plt.ylabel('$CO_2$(ppm) normalizado')\n",
        "plt.grid()\n",
        "plt.plot(range(len(train)),norm_train,'b')\n",
        "plt.plot(range(len(train),len(train)+len(test)),norm_test,'r')\n",
        "plt.legend(['Treinamento','Teste']);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q73akiYdzbbn"
      },
      "source": [
        "### Problema 2\n",
        "\n",
        "Podemos escrever uma função que separa os dados em um array de entrada (X) que tenha todo o dado no tempo t-i, e um array de saída que contenha os dados no momento t.\n",
        "\n",
        "A função usa dois argumentos: os dados e o look_back, que é o número de timesteps anteriores a serem usados como variáveis de entrada para prever o próximo período de tempo - o padrão é 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TkgSrO5zbbo"
      },
      "source": [
        "def split_X_y(data, look_back = 1, look_ahead=1):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data)-look_back-look_ahead):\n",
        "        val = data[i:i+look_back,0]\n",
        "        X.append(val)\n",
        "        y.append(data[i+look_back:i+look_back+look_ahead,0])\n",
        "    return np.array(X), np.array(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2SExHvgzbbr"
      },
      "source": [
        "# reshape em X contendo os dados para as amostras t-i ... t-2 t-1 e Y contendo as amostras t\n",
        "look_back = 1 # olha somente para a amostra anterior para prever a próxima amostra\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gc9wo5GSzbbv"
      },
      "source": [
        "A RNN simples espera que os dados de entrada (X) sejam fornecidos na forma de: [sample, time steps, features].\n",
        "\n",
        "Atualmente, os dados estão no formato: [samples, features], e estamos modelando o problema como um timestep para cada amostra. Usando numpy.reshape () da seguinte forma fazemos a devida transformação:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bas1Sphrzbby"
      },
      "source": [
        "# # reshape a entrada para [samples, time steps, features]\n",
        "def reshape_train_test(look_back, split_size):\n",
        "    trainX_whole, trainY_whole = split_X_y(norm_train, look_back)\n",
        "    testX, testY = split_X_y(norm_test, look_back)\n",
        "    trainX_whole = np.reshape(trainX_whole, (trainX_whole.shape[0], look_back, data.shape[1]))\n",
        "    testX = np.reshape(testX, (testX.shape[0], look_back, data.shape[1]))\n",
        "\n",
        "    # cria o dataset de validação\n",
        "    trainX, valX, trainY, valY = train_test_split(trainX_whole, trainY_whole, test_size=split_size)\n",
        "    return trainX_whole, trainX, valX, testX, trainY_whole, trainY, valY, testY"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQ9e92odzbb2"
      },
      "source": [
        "trainX_whole, trainX, valX, testX, trainY_whole, trainY, valY, testY = reshape_train_test(look_back, 0.7)\n",
        "\n",
        "print('Shape de x_train', trainX.shape)\n",
        "print('Shape de x_val', valX.shape)\n",
        "print('Shape de x_test', testX.shape)\n",
        "\n",
        "print('Shape de y_train', trainY.shape)\n",
        "print('Shape de y_val', valY.shape)\n",
        "print('Shape de y_test', testY.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-W-Cpf1Izbb6"
      },
      "source": [
        "Vamos agora projetar e ajustar nossa RNN simples.\n",
        "\n",
        "A rede tem uma camada visível com 1 entrada, uma camada oculta com 4 blocos recorrentes ou neurônios e uma camada de saída que faz uma previsão de valor único. A função de ativação sigmóide padrão é usada para os blocos recorrentes. A rede é treinada por 20 épocas e um tamanho de lote de 1 é usado."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vFbmyV6zbb7"
      },
      "source": [
        "### Rede RNN simples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "NotTDpvTzbb8"
      },
      "source": [
        "# cria e ajusta a RNN simples \n",
        "K.clear_session()\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(4, input_shape=(look_back,data.shape[1])))\n",
        "model.add(Dense(1))\n",
        "\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "history = model.fit(trainX, \n",
        "                    trainY, \n",
        "                    epochs=20, \n",
        "                    batch_size=1, \n",
        "                    verbose=1, \n",
        "                    validation_data=(valX, valY))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PybIM46DzbcB"
      },
      "source": [
        "Vamos agora definir uma função para fazer as previsões e plotar. \n",
        "Uma vez que o modelo estiver ajustado, podemos estimar o desempenho do modelo nos conjuntos de dados de treinamento e teste.\n",
        "\n",
        "Observe que devemos inverter (desnormalizar) as previsões antes de calcular a acurácia para garantir que o desempenho seja comparado nas mesmas unidades que os dados originais (ppm por mês).\n",
        "\n",
        "As previsões foram geradas usando o modelo para o conjunto de dados de treinamento e de teste. Também podemos visualizar os resultados para ter uma indicação de como o modelo funciona.\n",
        "\n",
        "Ao plotar os dados, devemos deslocar as previsões pelo look_back no tempo para alinhar no eixo x com o conjunto de dados original. Os dados são apresentados com o conjunto de dados original como pontos pretos, as previsões para o conjunto de dados de treinamento em azul e as previsões no conjunto de dados de teste em vermelho."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "_--7YAsgzbcD"
      },
      "source": [
        "def plot_history_predictions(history, Xtrain, Ytrain, Xtest, Ytest, scaler, model, title, xlabel, ylabel, lookback):\n",
        "    # Resumo do historico de loss\n",
        "    plt.figure(figsize=(20, 5))\n",
        "    plt.plot(history.history['loss'], color='blue')\n",
        "    plt.plot(history.history['val_loss'], color='red')\n",
        "    plt.title('Model loss', fontsize=20)\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['Treinamento', 'Validação'], loc='upper right', fontsize=14)\n",
        "    plt.show()\n",
        "\n",
        "    # faz as predições\n",
        "    trainPredict = model.predict(Xtrain)\n",
        "    testPredict = model.predict(Xtest)\n",
        "\n",
        "    # inverte as predições\n",
        "    trainPredict = scaler.inverse_transform(trainPredict)\n",
        "    trainYTrue = scaler.inverse_transform(Ytrain)\n",
        "    testPredict = scaler.inverse_transform(testPredict)\n",
        "    testYTrue = scaler.inverse_transform(Ytest)\n",
        "\n",
        "    # calcula o root mean squared error\n",
        "    trainScore = math.sqrt(mean_squared_error(trainYTrue, trainPredict))\n",
        "    print('Treinamento: %.2f RMSE' % (trainScore))\n",
        "    testScore = math.sqrt(mean_squared_error(testYTrue, testPredict))\n",
        "    print('Teste: %.2f RMSE' % (testScore))\n",
        "\n",
        "    plt.figure(figsize=[10,5])  \n",
        "    plt.title(title)\n",
        "    plt.xlabel(xlabel)\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.grid()\n",
        "    plt.plot(range(len(data)),data,'k.')\n",
        "    plt.plot(range(look_back,len(trainPredict)+look_back),trainPredict,'b')\n",
        "    plt.plot(range(len(trainPredict)+2*look_back,len(trainPredict)+len(testPredict)+2*look_back),testPredict,'r')\n",
        "    plt.legend(['Original','Treinamento','Teste']);\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "0dbAseygzbcH"
      },
      "source": [
        "plot_history_predictions(history, trainX_whole, trainY_whole, testX, testY, scaler, model, \n",
        "                         'Previsão com RNN simples', 'Meses', '$CO_2$(ppm)', look_back)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QK1Zo8f2zbcN"
      },
      "source": [
        "### Rede LSTM\n",
        "\n",
        "Vamos agora projetar e ajustar nossa rede LSTM.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "6JZUZn1vzbcP"
      },
      "source": [
        "# cria e ajusta a rede LSTM \n",
        "modelLSTM = Sequential()\n",
        "modelLSTM.add(LSTM(4, input_shape=(look_back, data.shape[1])))\n",
        "modelLSTM.add(Dense(1))\n",
        "\n",
        "modelLSTM.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "history = modelLSTM.fit(trainX, \n",
        "                    trainY, \n",
        "                    epochs=20, \n",
        "                    batch_size=1, \n",
        "                    verbose=1, \n",
        "                    validation_data=(valX, valY))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "bYbxqegKzbcU"
      },
      "source": [
        "plot_history_predictions(history, trainX_whole, trainY_whole, testX, testY, scaler, modelLSTM, \n",
        "                         'Previsão com LSTM', 'Meses', '$CO_2$(ppm)', look_back)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MoSpK3Yzbcd"
      },
      "source": [
        "### Problema 3\n",
        "\n",
        "Melhore os resultados acima tentando o seguinte:\n",
        "- Aumentar o número de épocas\n",
        "- Aumentar o valor look_back\n",
        "- Modificar a arquitetura\n",
        "- Outros..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tii-cfzzzbcf"
      },
      "source": [
        "Como exemplo, use look_back de 5, aumente o tamanho do dataset de treinamento, aumente o número de épocas e adicione uma camada de dropout\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByWJQK85zbci"
      },
      "source": [
        "look_back = 5 # olha somente para as amostras anteriores para prever a próxima amostra\n",
        "trainX_whole, trainX, valX, testX, trainY_whole, trainY, valY, testY = reshape_train_test(look_back, 0.7)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJQsLUKBzbcm"
      },
      "source": [
        "del modelLSTM\n",
        "modelLSTM = Sequential()\n",
        "modelLSTM.add(LSTM(4, input_shape=(look_back,data.shape[1])))\n",
        "modelLSTM.add(Dropout(0.2))\n",
        "modelLSTM.add(Dense(1))\n",
        "modelLSTM.compile(loss='mean_squared_error', optimizer='adam')\n",
        "history = modelLSTM.fit(trainX, \n",
        "                    trainY, \n",
        "                    epochs=20, \n",
        "                    batch_size=1, \n",
        "                    verbose=1, \n",
        "                    validation_data=(valX, valY))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4MH1_rfzbcq"
      },
      "source": [
        "plot_history_predictions(history, trainX_whole, trainY_whole, testX, testY, scaler, modelLSTM, \n",
        "                         'Previsão com LSTM', 'Meses', '$CO_2$(ppm)', look_back)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dxkvXc-zbcu"
      },
      "source": [
        "### Problema 4 - Rede GRU\n",
        "\n",
        "Utilizar outro modelo, a GRU\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNbMbCltzbcv"
      },
      "source": [
        "# reshape em X contendo os dados para as amostras t-i ... t-2 t-1 e Y contendo as amostras t\n",
        "look_back = 5 # olha somente para as amostras anteriores para prever a próxima amostra\n",
        "trainX_whole, trainX, valX, testX, trainY_whole, trainY, valY, testY = reshape_train_test(look_back, 0.7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "L89rooRPzbcz"
      },
      "source": [
        "modelGRU = Sequential()\n",
        "modelGRU.add(GRU(4, input_shape=(look_back, data.shape[1])))\n",
        "modelGRU.add(Dropout(0.2))\n",
        "modelGRU.add(Dense(1))\n",
        "modelGRU.compile(loss='mean_squared_error', optimizer='adam')\n",
        "history = modelGRU.fit(trainX, \n",
        "                    trainY, \n",
        "                    epochs=20, \n",
        "                    batch_size=1, \n",
        "                    verbose=1, \n",
        "                    validation_data=(valX, valY))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzfEEkLZzbc3"
      },
      "source": [
        "plot_history_predictions(history, trainX_whole, trainY_whole, testX, testY, scaler, modelGRU, \n",
        "                         'Previsão com GRU', 'Meses', '$CO_2$(ppm)', look_back)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IimHiChzbdE"
      },
      "source": [
        "### Problema 5 - Rede CNN1D\n",
        "\n",
        "Utilizar outro modelo, a CNN1D\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nu03BFTnzbdG"
      },
      "source": [
        "modelCNN1D = Sequential()\n",
        "modelCNN1D.add(Conv1D(filters=64, kernel_size=1, activation='tanh', input_shape=(look_back, data.shape[1])))\n",
        "modelCNN1D.add(Conv1D(filters=64, kernel_size=1, activation='tanh'))\n",
        "modelCNN1D.add(MaxPooling1D(pool_size=1))\n",
        "modelCNN1D.add(Dropout(0.2))\n",
        "modelCNN1D.add(Flatten())\n",
        "modelCNN1D.add(Dense(10, activation='tanh'))\n",
        "modelCNN1D.add(Dense(1, activation='linear'))\n",
        "\n",
        "modelCNN1D.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "vDVkyp4kzbdJ"
      },
      "source": [
        "tx = 0.001\n",
        "verbose = 1\n",
        "epochs = 20\n",
        "batch_size = 32\n",
        "\n",
        "perda = 'mae'\n",
        "metrica = 'mse'\n",
        "\n",
        "modelCNN1D.compile(loss=[perda], optimizer=tf.keras.optimizers.Adam(lr = tx), metrics=[metrica])\n",
        "\n",
        "history = modelCNN1D.fit(trainX, trainY, epochs=epochs, verbose=verbose, validation_data=(valX, valY), batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_v7AIPJnzbdN"
      },
      "source": [
        "plot_history_predictions(history, trainX_whole, trainY_whole, testX, testY, scaler, modelCNN1D, \n",
        "                         'Previsão com CNN1D', 'Meses', '$CO_2$(ppm)', look_back)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kz9rtcSqzbdQ"
      },
      "source": [
        "### Exercício\n",
        "\n",
        "Neste exercício, vamos construir um preditor de séries temporais e treiná-lo para prever uma única série temporal. Usaremos um conjunto de dados fornecido pelo [UCI Machine Learning Repository] (https://archive.ics.uci.edu/ml/datasets/PM2.5+Data+of+Five+Chinese+Cities) que possui dados de monitoramento da qualidade do ar nas cidades chinesas / distritos 1. Os dados se referem a concentração de material particulado de diametro menor que 2,5 micrometros (PM2.5), que são as partículas finas inaláveis pelo ser humano e perigosas a saúde. O período de monitoramento é de 01/01/2010 a 31/12/2015. Dados faltantes são denotados como NaN.\n",
        "\n",
        "Liang, X., S. Li, S. Zhang, H. Huang, and S. X. Chen (2016), PM2.5 data reliability, consistency, and air quality assessment in five Chinese cities, J. Geophys. Res. Atmos., 121, 10220 to 10236, [Web Link].\n",
        "\n",
        "\n",
        "#### Configurando os dados\n",
        "\n",
        "Começaremos trabalhando com dados de Pequim e filtraremos o conjunto de dados para registros a partir de 2015.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATqOEe1taUuv"
      },
      "source": [
        "### A atividade é prever para as próximas 6h o valor da concentração de PM2.5. \n",
        "\n",
        "Utilize os conceitos aprendidos até aqui, testando diferentes modelos de IA e diferentes abordagens para conseguir prever com o melhor desempenho o valor das próximas 6h da concentração de PM2.5 para uma série histórica de sua escolha.\n",
        "\n",
        "1. Inicialmente, escolha uma estação de monitoramento de uma das cidades com que irá trabalhar, e faça uma análise exploratória dos dados, avaliando estatisticamente e graficamente como os dados se comportam.\n",
        "2. Explore as técnicas MLP, RNN simples, LSTM, GRU, CNN1D, FCN+LSTM e Conv2DLSTM, utilizando a camada TimeDistributed;\n",
        "3. Escolha, com base na avaliação do loss e das métricas MAE, MSE, NMSE, r, R2 e Fac2 qual foi o melhor modelo;\n",
        "4. Use o ano de 2015 para fins de teste;\n",
        "5. Apresente os resultados de forma lógica, organizada e que seja reprodutível pelo professor;\n",
        "\n",
        "\n",
        "Importante: o trabalho deve ser feito em grupo. <br>\n",
        "\n",
        "Importante2: Conseguir utilizar as informações meteorológicas para melhorar a qualidade do modelo final;  <br>\n",
        "Importante3: Utilização de wavelets para feature augmentation. <br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download e preparação do arquivo:\n",
        "- Baixe o zip do link: https://archive.ics.uci.edu/ml/machine-learning-databases/00394/FiveCitiePMData.rar\n",
        "- Faça a extração do arquivo BeijingPM... e renomeie ele para Beijing.csv\n",
        "- Mova o arquivo para uma pasta no seu drive\n",
        "- Execute a próxima célula e autorize o acesso do Colab ao seu drive\n",
        "- insira o caminho do seu drive até o arquivo Beijing.csv, exemplo: \n",
        "\n",
        "`df_Beijing = pd.read_csv('/content/drive/MyDrive/IC/Beijing.csv')`"
      ],
      "metadata": {
        "id": "Mu0rzhcGeNNw"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wdXCWQS3k_F"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "4j-pZpvWzbdR"
      },
      "source": [
        "df_Beijing = pd.read_csv('./sample_data/Beijing.csv')\n",
        "df_Beijing = df_Beijing[df_Beijing.year >= 2015]\n",
        "df_Beijing.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfWpsN7LzbdY"
      },
      "source": [
        "# interpolando os dados\n",
        "df_Beijing['PM_Dongsi'] = df_Beijing['PM_Dongsi'].interpolate()\n",
        "df_Beijing['PM_Dongsi'].head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4hsHM5xzbdd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}